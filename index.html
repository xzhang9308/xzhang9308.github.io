<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Xi Zhang </title> <meta name="author" content="Xi Zhang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://xzhang9308.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/arxiv/">arXiv </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Resume </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Xi</span> Zhang </h1> <p class="desc"><a href="https://www.ntu.edu.sg/news/detail/new-alibaba-ntu-corporate-lab-to-advance-green-digital-technologies" rel="external nofollow noopener" target="_blank">Research Scientist, ANGEL Lab, Nanyang Technological University (NTU), Singapore</a>.</p> </header> <article> <div class="profile float-right" style="--profile-img-max: 210px"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/xzhang3-480.webp 480w,/assets/img/xzhang3-800.webp 800w,/assets/img/xzhang3-1400.webp 1400w," type="image/webp" sizes="(min-width: 1100px) 235.4px, (min-width: 576px) 22.0vw, 80vw"> <img src="/assets/img/xzhang3.jpeg?dd057c734fc00ab4df14912ff120f90b" class="img-fluid z-depth-1 rounded-circle" width="100%" height="auto" alt="xzhang3.jpeg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <p>ABN-03b-07/08</p> <p>Academic Block North</p> <p>61 Nanyang Dr, Singapore</p> </div> </div> <div class="clearfix"> <p>I am currently a Research Scientist in the Alibaba-NTU Global e-Sustainability CorpLab (ANGEL) at Nanyang Technological University (NTU), working with Prof <a href="https://personal.ntu.edu.sg/wslin/Home.html" rel="external nofollow noopener" target="_blank">Weisi Lin</a>. Before that, I was a postdoctoral fellow at McMaster University, Canada from July 2022 to August 2024, supervised by Prof <a href="https://scholar.google.com/citations?user=ZuQnEIgAAAAJ" rel="external nofollow noopener" target="_blank">Xiaolin Wu</a>.<br> I received my Ph.D. in Electrical Engineering from Shanghai Jiao Tong University (SJTU) in June 2022, and my bachelor’s degree in Mathematics and Physics Basic Science from University of Electronic Science and Technology of China (UESTC) in 2015. </p> <p>My current research focuses on Green AI, particularly on efficient model design, sustainable system architectures, and resource-aware compression techniques. In this line, I work on lightweight architectures, quantization, and compression methods to reduce energy, memory, and computational costs of large-scale models while maintaining strong performance. Previously, my research centered on learning-based data compression for various visual modalities such as images and videos. I am also interested in broader challenges in deep learning, including domain generalization and visual reasoning.</p> </div> <hr style="margin: 20px 0;"> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Oct 17, 2025</th> <td> I was selected as a <span class="news-badge news-badge-conference">NeurIPS 2025</span> <span class="news-badge news-badge-honor">Top Reviewer</span>. </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 18, 2025</th> <td> Two papers on Green AI are accepted by <span class="news-badge news-badge-conference">NeurIPS 2025</span>. </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 05, 2025</th> <td> Our <span class="news-badge news-badge-conference">CVPR 2025</span> paper has been selected as a <span class="news-badge news-badge-highlight">highlight</span> (Top 3%). </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 26, 2025</th> <td> One paper on multirate image compression is accepted by <span class="news-badge news-badge-conference">CVPR 2025</span>. </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 30, 2024</th> <td> One paper on optimal lattice vector quantizer is accepted by <span class="news-badge news-badge-conference">NeurIPS 2024</span>. </td> </tr> </table> </div> <div class="news-actions text-right mt-2"> <a class="btn btn-sm btn-outline-primary" href="/news/"> Show all news </a> </div> </div> <hr style="margin: 20px 0;"> <h2> <a href="/publications/" style="color: inherit">Selected Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-lg-2 col-md-3 col-sm-4 abbr"> <abbr class="badge rounded w-100">NeurIPS 2025</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/glvq-480.webp 480w,/assets/img/publication_preview/glvq-800.webp 800w,/assets/img/publication_preview/glvq-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/glvq.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="glvq.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zhang2025learning" class="col-lg-10 col-md-9 col-sm-8"> <div class="title">Learning Grouped Lattice Vector Quantizers for Low-Bit LLM Compression</div> <div class="author"> <em>Xi Zhang</em>, <a href="https://scholar.google.com/citations?user=ZuQnEIgAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Xiaolin Wu</a>, <a href="https://openreview.net/profile?id=~Jiamang_Wang1" rel="external nofollow noopener" target="_blank">Jiamang Wang</a>, and <a href="https://scholar.google.com/citations?user=D_S41X4AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Weisi Lin</a> </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/html/" class="btn btn-sm z-depth-0" role="button">HTML</a> </div> <div class="abstract hidden"> <p>Large Language Models (LLMs) have demonstrated remarkable capabilities but typically require extensive computational resources and memory for inference. Post-training quantization (PTQ) can effectively reduce these demands by storing weights in lower bit-width formats. However, standard uniform quantization often leads to notable performance degradation, particularly in low-bit scenarios. In this work, we introduce a Grouped Lattice Vector Quantization (GLVQ) framework that assigns each group of weights a customized lattice codebook, defined by a learnable generation matrix. To address the non-differentiability of the quantization process, we adopt Babai rounding to approximate nearest-lattice-point search during training, which enables stable optimization of the generation matrices. Once trained, decoding reduces to a simple matrix-vector multiplication, yielding an efficient and practical quantization pipeline. Experiments on multiple benchmarks show that our approach achieves a better trade-off between model size and accuracy compared to existing post-training quantization baselines, highlighting its effectiveness in deploying large models under stringent resource constraints.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2025learning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning Grouped Lattice Vector Quantizers for Low-Bit LLM Compression}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Xi and Wu, Xiaolin and Wang, Jiamang and Lin, Weisi}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-lg-2 col-md-3 col-sm-4 abbr"> <abbr class="badge rounded w-100">NeurIPS 2025</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/badiff-480.webp 480w,/assets/img/publication_preview/badiff-800.webp 800w,/assets/img/publication_preview/badiff-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/badiff.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="badiff.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zhang2025badiff" class="col-lg-10 col-md-9 col-sm-8"> <div class="title">BADiff: Bandwidth Adaptive Diffusion Model</div> <div class="author"> <em>Xi Zhang</em>, <a href="https://openreview.net/profile?id=~Hanwei_Zhu1" rel="external nofollow noopener" target="_blank">Hanwei Zhu</a>, <a href="https://openreview.net/profile?id=~Yan_Zhong2" rel="external nofollow noopener" target="_blank">Yan Zhong</a>, <a href="https://openreview.net/profile?id=~Jiamang_Wang1" rel="external nofollow noopener" target="_blank">Jiamang Wang</a>, and <a href="https://scholar.google.com/citations?user=D_S41X4AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Weisi Lin</a> </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/html/" class="btn btn-sm z-depth-0" role="button">HTML</a> </div> <div class="abstract hidden"> <p> In this work, we propose a novel framework to enable diffusion models to adapt their generation quality based on real-time network bandwidth constraints. Traditional diffusion models produce high-fidelity images by performing a fixed number of denoising steps, regardless of downstream transmission limitations. However, in practical cloud-to-device scenarios, limited bandwidth often necessitates heavy compression, leading to loss of fine textures and wasted computation. To address this, we introduce a joint end-to-end training strategy where the diffusion model is conditioned on a target quality level derived from the available bandwidth. During training, the model learns to adaptively modulate the denoising process, enabling early-stop sampling that maintains perceptual quality appropriate to the target transmission condition. Our method requires minimal architectural changes and leverages a lightweight quality embedding to guide the denoising trajectory. Experimental results demonstrate that our approach significantly improves the visual fidelity of bandwidth-adapted generations compared to naive early-stopping, offering a promising solution for efficient image delivery in bandwidth-constrained environments.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2025badiff</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{BADiff: Bandwidth Adaptive Diffusion Model}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Xi and Zhu, Hanwei and Zhong, Yan and Wang, Jiamang and Lin, Weisi}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-lg-2 col-md-3 col-sm-4 abbr"> <abbr class="badge rounded w-100">CVPR 2025 - Highlight</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mlvq-480.webp 480w,/assets/img/publication_preview/mlvq-800.webp 800w,/assets/img/publication_preview/mlvq-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/mlvq.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mlvq.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="xu2025multirate" class="col-lg-10 col-md-9 col-sm-8"> <div class="title">Multirate Neural Image Compression with Adaptive Lattice Vector Quantization</div> <div class="author"> <a href="https://scholar.google.ca/citations?user=4uhsUhEAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Hao Xu</a>, <a href="https://scholar.google.com/citations?user=ZuQnEIgAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Xiaolin Wu</a>, and <em>Xi Zhang</em> </div> <div class="periodical"> <em>Proceedings of the Computer Vision and Pattern Recognition Conference</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Multirate_Neural_Image_Compression_with_Adaptive_Lattice_Vector_Quantization_CVPR_2025_paper.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Recent research has explored integrating lattice vector quantization (LVQ) into learned image compression models. Due to its more efficient Voronoi covering of vector space than scalar quantization (SQ), LVQ achieves better rate-distortion (R-D) performance than SQ, while still retaining the low complexity advantage of SQ. However, existing LVQ-based methods have two shortcomings: 1) lack of a multirate coding mode, hence incapable to operate at different rates; 2) the use of a fixed lattice basis, hence nonadaptive to changing source distributions. To overcome these shortcomings, we propose a novel adaptive LVQ method, which is the first among LVQ-based methods to achieve both rate and domain adaptations. By scaling the lattice basis vector, our method can adjust the density of lattice points to achieve various bit rate targets, achieving superior R-D performance to current SQ-based variable rate models. Additionally, by using a learned invertible linear transformation between two different input domains, we can reshape the predefined lattice cell to better represent the target domain, further improving the R-D performance. To our knowledge, this paper represents the first attempt to propose a unified solution for rate adaptation and domain adaptation through quantizer design.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu2025multirate</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multirate Neural Image Compression with Adaptive Lattice Vector Quantization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Hao and Wu, Xiaolin and Zhang, Xi}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Computer Vision and Pattern Recognition Conference}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{7633--7642}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-lg-2 col-md-3 col-sm-4 abbr"> <abbr class="badge rounded w-100">NeurIPS 2024</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/olvq-480.webp 480w,/assets/img/publication_preview/olvq-800.webp 800w,/assets/img/publication_preview/olvq-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/olvq.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="olvq.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zhang2024learning" class="col-lg-10 col-md-9 col-sm-8"> <div class="title">Learning Optimal Lattice Vector Quantizers for End-to-end Neural Image Compression</div> <div class="author"> <em>Xi Zhang</em>, and <a href="https://scholar.google.com/citations?user=ZuQnEIgAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Xiaolin Wu</a> </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2411.16119" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openreview.net/forum?id=dLr4H7Uj4H&amp;referrer=%5Bthe%20profile%20of%20Xiaolin%20Wu%5D(%2Fprofile%3Fid%3D%C2%A0Xiaolin_Wu2)" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/xzhang9308/OLVQ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>It is customary to deploy uniform scalar quantization in the end-to-end optimized Neural image compression methods, instead of more powerful vector quantization, due to the high complexity of the latter. Lattice vector quantization (LVQ), on the other hand, presents a compelling alternative, which can exploit inter-feature dependencies more effectively while keeping computational efficiency almost the same as scalar quantization. However, traditional LVQ structures are designed/optimized for uniform source distributions, hence nonadaptive and suboptimal for real source distributions of latent code space for Neural image compression tasks. In this paper, we propose a novel learning method to overcome this weakness by designing the rate-distortion optimal lattice vector quantization (OLVQ) codebooks with respect to the sample statistics of the latent features to be compressed. By being able to better fit the LVQ structures to any given latent sample distribution, the proposed OLVQ method improves the rate-distortion performances of the existing quantization schemes in neural image compression significantly, while retaining the amenability of uniform scalar quantization.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2024learning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning Optimal Lattice Vector Quantizers for End-to-end Neural Image Compression}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Xi and Wu, Xiaolin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{106497--106518}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{37}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-lg-2 col-md-3 col-sm-4 abbr"> <abbr class="badge rounded w-100">ECCV 2024</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/crcir-480.webp 480w,/assets/img/publication_preview/crcir-800.webp 800w,/assets/img/publication_preview/crcir-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/crcir.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="crcir.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="xu2024fast" class="col-lg-10 col-md-9 col-sm-8"> <div class="title">Fast Point Cloud Geometry Compression with Context-Based Residual Coding and INR-Based Refinement</div> <div class="author"> <a href="https://scholar.google.ca/citations?user=4uhsUhEAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Hao Xu</a>, <em>Xi Zhang</em>, and <a href="https://scholar.google.com/citations?user=ZuQnEIgAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Xiaolin Wu</a> </div> <div class="periodical"> <em>European Conference on Computer Vision</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2408.02966" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/chapter/10.1007/978-3-031-73113-6_16" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/hxu160/CRCIR_for_PCGC" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Compressing a set of unordered points is far more challenging than compressing images/videos of regular sample grids, because of the difficulties in characterizing neighboring relations in an irregular layout of points. Many researchers resort to voxelization to introduce regularity, but this approach suffers from quantization loss. In this research, we use the KNN method to determine the neighborhoods of raw surface points. This gives us a means to determine the spatial context in which the latent features of 3D points are compressed by arithmetic coding. As such, the conditional probability model is adaptive to local geometry, leading to significant rate reduction. Additionally, we propose a dual-layer architecture where a non-learning base layer reconstructs the main structures of the point cloud at low complexity, while a learned refinement layer focuses on preserving fine details. This design leads to reductions in model complexity and coding latency by two orders of magnitude compared to SOTA methods. Moreover, we incorporate an implicit neural representation (INR) into the refinement layer, allowing the decoder to sample points on the underlying surface at arbitrary densities. This work is the first to effectively exploit content-aware local contexts for compressing irregular raw point clouds, achieving high rate-distortion performance, low complexity, and the ability to function as an arbitrary-scale upsampling network simultaneously.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu2024fast</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Fast Point Cloud Geometry Compression with Context-Based Residual Coding and INR-Based Refinement}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Hao and Zhang, Xi and Wu, Xiaolin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{European Conference on Computer Vision}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{270--288}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-lg-2 col-md-3 col-sm-4 abbr"> <abbr class="badge rounded w-100">JVCI 2024</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/infty4d-480.webp 480w,/assets/img/publication_preview/infty4d-800.webp 800w,/assets/img/publication_preview/infty4d-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/infty4d.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="infty4d.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="mukati2024low" class="col-lg-10 col-md-9 col-sm-8"> <div class="title">Low-complexity ℓ∞-compression of light field images with a deep-decompression stage</div> <div class="author"> <a href="https://umairmukati.github.io/" rel="external nofollow noopener" target="_blank">M Umair Mukati</a>, <em>Xi Zhang</em>, <a href="https://scholar.google.com/citations?user=ZuQnEIgAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Xiaolin Wu</a>, and <a href="https://orbit.dtu.dk/en/persons/s%C3%B8ren-otto-forchhammer" rel="external nofollow noopener" target="_blank">Søren Forchhammer</a> </div> <div class="periodical"> <em>Journal of Visual Communication and Image Representation</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2201.09834" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S1047320324000270" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/umairmukati/deep-linf-epic" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>To enrich the functionalities of traditional cameras, light field cameras record both the intensity and direction of light rays, so that images can be rendered with user-defined camera parameters via computations. The added capability and flexibility are gained at the cost of gathering typically more than 100 perspectives of the same scene, resulting in large data volume. To cope with this issue, several light field compression schemes have been introduced. However, their ways of exploiting correlations of multidimensional light field data are complex and are hence not suited for cost-effective light field cameras. On the other hand, existing simpler compression schemes do not offer good compression performance. In this work, we propose a novel 𝓁∞-constrained light-field image compression system that has a very low-complexity DPCM encoder and a CNN-based deep decoder enhancement. Targeting high-fidelity soft-decoding (restoration), the CNN decoding capitalizes on the 𝓁∞-constraint, i.e. the maximum absolute error bound, and light field properties to remove the compression artifacts. Two different architectures for CNN decoder enhancement are proposed, one is based on 2D CNNs and optimized for fast inference, and another is based on 3D CNNs to maximize 𝓁2 restoration quality. The proposed networks achieve superior performance both in inference speed and restoration quality in comparison to state-of-the-art light field network architectures. In conjunction with 𝓁∞-EPIC, the proposed architecture, while satisfying a well-defined 𝓁∞ constraint, outperforms existing state-of-the-art 𝓁2-based light field compression methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">mukati2024low</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Low-complexity ℓ∞-compression of light field images with a deep-decompression stage}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mukati, M Umair and Zhang, Xi and Wu, Xiaolin and Forchhammer, S{\o}ren}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Visual Communication and Image Representation}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{99}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{104072}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-lg-2 col-md-3 col-sm-4 abbr"> <abbr class="badge rounded w-100">CVPR 2023</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/lvqac-480.webp 480w,/assets/img/publication_preview/lvqac-800.webp 800w,/assets/img/publication_preview/lvqac-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/lvqac.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="lvqac.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zhang2023lvqac" class="col-lg-10 col-md-9 col-sm-8"> <div class="title">Lvqac: Lattice vector quantization coupled with spatially adaptive companding for efficient learned image compression</div> <div class="author"> <em>Xi Zhang</em>, and <a href="https://scholar.google.com/citations?user=ZuQnEIgAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Xiaolin Wu</a> </div> <div class="periodical"> <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2304.12319" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_LVQAC_Lattice_Vector_Quantization_Coupled_With_Spatially_Adaptive_Companding_for_CVPR_2023_paper.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Recently, numerous end-to-end optimized image compression neural networks have been developed and proved themselves as leaders in rate-distortion performance. The main strength of these learnt compression methods is in powerful nonlinear analysis and synthesis transforms that can be facilitated by deep neural networks. However, out of operational expediency, most of these end-to-end methods adopt uniform scalar quantizers rather than vector quantizers, which are information-theoretically optimal. In this paper, we present a novel Lattice Vector Quantization scheme coupled with a spatially Adaptive Companding (LVQAC) mapping. LVQ can better exploit the inter-feature dependencies than scalar uniform quantization while being computationally almost as simple as the latter. Moreover, to improve the adaptability of LVQ to source statistics, we couple a spatially adaptive companding (AC) mapping with LVQ. The resulting LVQAC design can be easily embedded into any end-to-end optimized image compression system. Extensive experiments demonstrate that for any end-to-end CNN image compression models, replacing uniform quantizer by LVQAC achieves better rate-distortion performance without significantly increasing the model complexity.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2023lvqac</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Lvqac: Lattice vector quantization coupled with spatially adaptive companding for efficient learned image compression}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Xi and Wu, Xiaolin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/CVF Conference on Computer Vision and Pattern Recognition}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{10239--10248}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-lg-2 col-md-3 col-sm-4 abbr"> <abbr class="badge rounded w-100">TPAMI 2022</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mdvd-480.webp 480w,/assets/img/publication_preview/mdvd-800.webp 800w,/assets/img/publication_preview/mdvd-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/mdvd.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mdvd.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zhang2022multi" class="col-lg-10 col-md-9 col-sm-8"> <div class="title">Multi-modality deep restoration of extremely compressed face videos</div> <div class="author"> <em>Xi Zhang</em>, and <a href="https://scholar.google.com/citations?user=ZuQnEIgAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Xiaolin Wu</a> </div> <div class="periodical"> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2107.05548" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9730053" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Arguably the most common and salient object in daily video communications is the talking head, as encountered in social media, virtual classrooms, teleconferences, news broadcasting, talk shows, etc. When communication bandwidth is limited by network congestions or cost effectiveness, compression artifacts in talking head videos are inevitable. The resulting video quality degradation is highly visible and objectionable due to high acuity of human visual system to faces. To solve this problem, we develop a multi-modality deep convolutional neural network method for restoring face videos that are aggressively compressed. The main innovation is a new DCNN architecture that incorporates known priors of multiple modalities: the video-synchronized speech signal and semantic elements of the compression code stream, including motion vectors, code partition map and quantization parameters. These priors strongly correlate with the latent video and hence they are able to enhance the capability of deep learning to remove compression artifacts. Ample empirical evidences are presented to validate the superior performance of the proposed DCNN method on face videos over the existing state-of-the-art methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhang2022multi</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multi-modality deep restoration of extremely compressed face videos}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Xi and Wu, Xiaolin}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Pattern Analysis and Machine Intelligence}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{45}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2024--2037}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-lg-2 col-md-3 col-sm-4 abbr"> <abbr class="badge rounded w-100">CVPR 2021</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/agdl-480.webp 480w,/assets/img/publication_preview/agdl-800.webp 800w,/assets/img/publication_preview/agdl-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/agdl.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="agdl.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zhang2021attention" class="col-lg-10 col-md-9 col-sm-8"> <div class="title">Attention-guided image compression by deep reconstruction of compressive sensed saliency skeleton</div> <div class="author"> <em>Xi Zhang</em>, and <a href="https://scholar.google.com/citations?user=ZuQnEIgAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Xiaolin Wu</a> </div> <div class="periodical"> <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2103.15368" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Attention-Guided_Image_Compression_by_Deep_Reconstruction_of_Compressive_Sensed_Saliency_CVPR_2021_paper.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>We propose a deep learning system for attention-guided dual-layer image compression (AGDL). In the AGDL compression system, an image is encoded into two layers, a base layer and an attention-guided refinement layer. Unlike the existing ROI image compression methods that spend an extra bit budget equally on all pixels in ROI, AGDL employs a CNN module to predict those pixels on and near a saliency sketch within ROI that are critical to perceptual quality. Only the critical pixels are further sampled by compressive sensing (CS) to form a very compact refinement layer. Another novel CNN method is developed to jointly decode the two compression layers for a much refined reconstruction, while strictly satisfying the transmitted CS constraints on perceptually critical pixels. Extensive experiments demonstrate that the proposed AGDL system advances the state of the art in perception-aware image compression.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2021attention</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Attention-guided image compression by deep reconstruction of compressive sensed saliency skeleton}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Xi and Wu, Xiaolin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/CVF Conference on Computer Vision and Pattern Recognition}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{13354--13364}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-lg-2 col-md-3 col-sm-4 abbr"> <abbr class="badge rounded w-100">TIP 2021</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/inftyed2-480.webp 480w,/assets/img/publication_preview/inftyed2-800.webp 800w,/assets/img/publication_preview/inftyed2-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/inftyed2.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="inftyed2.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zhang2020ultra" class="col-lg-10 col-md-9 col-sm-8"> <div class="title">Ultra high fidelity deep image decompression with l∞-constrained compression</div> <div class="author"> <em>Xi Zhang</em>, and <a href="https://scholar.google.com/citations?user=ZuQnEIgAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Xiaolin Wu</a> </div> <div class="periodical"> <em>IEEE Transactions on Image Processing</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2002.03482" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9277919" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>In many professional fields, such as medicine, remote sensing and sciences, users often demand image compression methods to be mathematically lossless. But lossless image coding has a rather low compression ratio (around 2:1 for natural images). The only known technique to achieve significant compression while meeting the stringent fidelity requirements is the methodology of ℓ∞-constrained coding that was developed and standardized in nineties. We make a major progress in ℓ∞-constrained image coding after two decades, by developing a novel CNN-based soft ℓ∞-constrained decoding method. The new method repairs compression defects by using a restoration CNN called the ℓ∞-SDNet to map a conventionally decoded image to the latent image. A unique strength of the ℓ∞-SDNet is its ability to enforce a tight error bound on a per pixel basis. As such, no small distinctive structures of the original image can be dropped or distorted, even if they are statistical outliers that are otherwise sacrificed by mainstream CNN restoration methods. More importantly, this research ushers in a new image compression system of ℓ∞-constrained encoding and deep soft decoding (ℓ∞-ED2). The ℓ∞-ED2 approach beats the best of existing lossy image compression methods (e.g., BPG, WebP, etc.) not only in ℓ∞ but also in ℓ2 error metric and perceptual quality, for bit rates near the threshold of perceptually transparent reconstruction. Operationally, the new compression system is practical, with a low-complexity real-time encoder and a cascade decoder consisting of a fast initial decoder and an optional CNN soft decoder.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhang2020ultra</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Ultra high fidelity deep image decompression with l∞-constrained compression}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Xi and Wu, Xiaolin}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Image Processing}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{30}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{963--975}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-lg-2 col-md-3 col-sm-4 abbr"> <abbr class="badge rounded w-100">NeurIPS 2020</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/numerosity-480.webp 480w,/assets/img/publication_preview/numerosity-800.webp 800w,/assets/img/publication_preview/numerosity-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/numerosity.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="numerosity.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zhang2020numerosity" class="col-lg-10 col-md-9 col-sm-8"> <div class="title">On numerosity of deep neural networks</div> <div class="author"> <em>Xi Zhang</em>, and <a href="https://scholar.google.com/citations?user=ZuQnEIgAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Xiaolin Wu</a> </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2011.08674" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://papers.nips.cc/paper_files/paper/2020/hash/13e36f06c66134ad65f532e90d898545-Abstract.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Recently, a provocative claim was published that number sense spontaneously emerges in a deep neural network trained merely for visual object recognition. This has, if true, far reaching significance to the fields of machine learning and cognitive science alike. In this paper, we prove the above claim to be unfortunately incorrect. The statistical analysis to support the claim is flawed in that the sample set used to identify number-aware neurons is too small, compared to the huge number of neurons in the object recognition network. By this flawed analysis one could mistakenly identify number-sensing neurons in any randomly initialized deep neural networks that are not trained at all. With the above critique we ask the question what if a deep convolutional neural network is carefully trained for numerosity? Our findings are mixed. Even after being trained with number-depicting images, the deep learning approach still has difficulties to acquire the abstract concept of numbers, a cognitive task that preschoolers perform with ease. But on the other hand, we do find some encouraging evidences suggesting that deep neural networks are more robust to distribution shift for small numbers than for large numbers.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2020numerosity</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On numerosity of deep neural networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Xi and Wu, Xiaolin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{33}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1820--1829}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-lg-2 col-md-3 col-sm-4 abbr"> <abbr class="badge rounded w-100">CVPR 2020 - Oral</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/davd-480.webp 480w,/assets/img/publication_preview/davd-800.webp 800w,/assets/img/publication_preview/davd-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/davd.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="davd.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zhang2020davd" class="col-lg-10 col-md-9 col-sm-8"> <div class="title">Davd-net: Deep audio-aided video decompression of talking heads</div> <div class="author"> <em>Xi Zhang</em>, <a href="https://scholar.google.com/citations?user=ZuQnEIgAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Xiaolin Wu</a>, <a href="https://scholar.google.com/citations?user=Gh2E1tQAAAAJ&amp;hl=zh-CN" rel="external nofollow noopener" target="_blank">Xinliang Zhai</a>, <a href="https://ieeexplore.ieee.org/author/37680015700" rel="external nofollow noopener" target="_blank">Xianye Ben</a>, and <a href="https://ieeexplore.ieee.org/author/37334594400" rel="external nofollow noopener" target="_blank">Chengjie Tu</a> </div> <div class="periodical"> <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_DAVD-Net_Deep_Audio-Aided_Video_Decompression_of_Talking_Heads_CVPR_2020_paper.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Close-up talking heads are among the most common and salient object in video contents, such as face-to-face conversations in social media, teleconferences, news broadcasting, talk shows, etc. Due to the high sensitivity of human visual system to faces, compression distortions in talking heads videos are highly visible and annoying. To address this problem, we present a novel deep convolutional neural network (DCNN) method for very low bit rate video reconstruction of talking heads. The key innovation is a new DCNN architecture that can exploit the audio-video correlations to repair compression defects in the face region. We further improve reconstruction quality by embedding into our DCNN the encoder information of the video compression standards and introducing a constraining projection module in the network. Extensive experiments demonstrate that the proposed DCNN method outperforms the existing state-of-the-art methods on videos of talking heads.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2020davd</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Davd-net: Deep audio-aided video decompression of talking heads}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Xi and Wu, Xiaolin and Zhai, Xinliang and Ben, Xianye and Tu, Chengjie}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/CVF Conference on Computer Vision and Pattern Recognition}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{12335--12344}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-lg-2 col-md-3 col-sm-4 abbr"> <abbr class="badge rounded w-100">ACM MM 2020</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mmsd-480.webp 480w,/assets/img/publication_preview/mmsd-800.webp 800w,/assets/img/publication_preview/mmsd-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/mmsd.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mmsd.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="guo2020deep" class="col-lg-10 col-md-9 col-sm-8"> <div class="title">Deep multi-modality soft-decoding of very low bit-rate face videos</div> <div class="author"> <a href="https://scholar.google.com/citations?user=XwxwxfQAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Yanhui Guo</a>, <em>Xi Zhang</em>, and <a href="https://scholar.google.com/citations?user=ZuQnEIgAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Xiaolin Wu</a> </div> <div class="periodical"> <em>ACM International Conference on Multimedia</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2008.01652" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dl.acm.org/doi/10.1145/3394171.3413709" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>We propose a novel deep multi-modality neural network for restoring very low bit rate videos of talking heads. Such video contents are very common in social media, teleconferencing, distance education, tele-medicine, etc., and often need to be transmitted with limited bandwidth. The proposed CNN method exploits the correlations among three modalities, video, audio and emotion state of the speaker, to remove the video compression artifacts caused by spatial down sampling and quantization. The deep learning approach turns out to be ideally suited for the video restoration task, as the complex non-linear cross-modality correlations are very difficult to model analytically and explicitly. The new method is a video post processor that can significantly boost the perceptual quality of aggressively compressed talking head videos, while being fully compatible with all existing video compression standards.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">guo2020deep</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Deep multi-modality soft-decoding of very low bit-rate face videos}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Guo, Yanhui and Zhang, Xi and Wu, Xiaolin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ACM International Conference on Multimedia}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3947--3955}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-lg-2 col-md-3 col-sm-4 abbr"> <abbr class="badge rounded w-100">AAAI 2019 - Spotlight</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/deficit-480.webp 480w,/assets/img/publication_preview/deficit-800.webp 800w,/assets/img/publication_preview/deficit-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/deficit.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="deficit.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="wu2019cognitive" class="col-lg-10 col-md-9 col-sm-8"> <div class="title">Cognitive deficit of deep learning in numerosity</div> <div class="author"> <em>Xi Zhang</em>, <a href="https://scholar.google.com/citations?user=ZuQnEIgAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Xiaolin Wu</a>, and <a href="https://scholar.google.com/citations?user=5qcvYsoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Xiao Shu</a> </div> <div class="periodical"> <em>AAAI conference on artificial intelligence</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1802.05160" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/3928" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Subitizing, or the sense of small natural numbers, is an innate cognitive function of humans and primates; it responds to visual stimuli prior to the development of any symbolic skills, language or arithmetic. Given successes of deep learning (DL) in tasks of visual intelligence and given the primitivity of number sense, a tantalizing question is whether DL can comprehend numbers and perform subitizing. But somewhat disappointingly, extensive experiments of the type of cognitive psychology demonstrate that the examples-driven black box DL cannot see through superficial variations in visual representations and distill the abstract notion of natural number, a task that children perform with high accuracy and confidence. The failure is apparently due to the learning method not the CNN computational machinery itself. A recurrent neural network capable of subitizing does exist, which we construct by encoding a mechanism of mathematical morphology into the CNN convolutional kernels. Also, we investigate, using subitizing as a test bed, the ways to aid the black box DL by cognitive priors derived from human insight. Our findings are mixed and interesting, pointing to both cognitive deficit of pure DL, and some measured successes of boosting DL by predetermined cognitive implements. This case study of DL in cognitive computing is meaningful for visual numerosity represents a minimum level of human intelligence.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wu2019cognitive</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Cognitive deficit of deep learning in numerosity}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Xi and Wu, Xiaolin and Shu, Xiao}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{AAAI conference on artificial intelligence}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{33}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{01}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1303--1310}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="https://scholar.google.com/citations?user=78WvEjMAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://orcid.org/0000-0002-1993-6031" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://ieeexplore.ieee.org/author/37086343380/" title="IEEE Xplore" rel="external nofollow noopener" target="_blank"><i class="ai ai-ieee"></i></a> <a href="https://github.com/xzhang9308" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="mailto:%78%7A%68%61%6E%67%39%33%30%38@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> </div> <div class="contact-note"></div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Xi Zhang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>